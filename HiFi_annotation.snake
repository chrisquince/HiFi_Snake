import glob
import pysam
import numpy as np
from collections import defaultdict,Counter
from os.path import basename,dirname,realpath,abspath
from Bio.SeqIO.FastaIO import SimpleFastaParser as sfp


# same as HIFI_annoataiotn_on_graph but without graph

SCRIPTS = "/mnt/gpfs/seb/Project/Metahood/scripts"
SCG_DATA = "/mnt/gpfs/seb/Project/Metahood/scg_data"
CONDA = "/home/sebr/seb/Applications/miniconda3"
CONDA_ENV = "/mnt/gpfs/seb/Project/Metahood/conda_envs"
BLCA = "/home/sebr/seb/Applications/BLCA"
COG_DB = "/mnt/gpfs/seb/Database/rpsblast_cog_db/Cog"
PLASMIDNET = "/mnt/gpfs/seb/Applications/PlasmidNet"
VIRSORTER_ENV = "/mnt/gpfs/seb/Applications/miniconda3/envs/virsort2"
rRNA_DB = "/mnt/gpfs/seb/Database/Silva_27_08_2020/smr_v4.3_sensitive_db_rfam_seeds.fasta"
DIAMOND= {"CARD":{"db":"/mnt/gpfs/seb/Database/CARD/card_5_10_21/CARD_51021_2.0.8.dmnd","annotation":"/mnt/gpfs/seb/Database/CARD/card_5_10_21/card_5_10_21_Annotation.tsv","filter": [0,1e-5,0.5,0,0.2,0.5]}}

ROOT = config["ROOT"]
CONTIGS = config["CONTIGS"]
PID = "0.99"

ruleorder: parse_cogs_annotation > annotation_diamond

rule results:
    input: "%s/annotation/16S_blca.tsv"%ROOT,
           "%s/annotation/SCG_cluster_%s.tsv"%(ROOT,PID),
           "%s/annotation/contigs_CARD_best_hits.tsv"%ROOT,
           "%s/plasmidnet/results.tsv"%ROOT,
           "%s/virsorter2/final-viral-boundary.tsv"%ROOT,
           "%s/annotation/summary_0.99.tsv"%ROOT



rule symlink_contigs:
	output: "{path}/contigs/contigs.fa"
	run:
		if CONTIGS.endswith(".gz"):
			shell("zcat {CONTIGS} > {output}")
		else:
			shell("ln -s {CONTIGS} {output}")


rule prodigal:
    input:
        "{path}/contigs/contigs.fa"
    output:
        faa="{path}/annotation/contigs.faa",
        fna="{path}/annotation/contigs.fna",
        gff="{path}/annotation/contigs.gff",
        cut_faa=expand("{{path}}/annotation/temp_splits/Batch_{nb}.faa",nb=range(100))
    params:
        dir='{path}/annotation'
    threads:
        1000
    message:"Parallel prodigal run in {input}"
    shell:
        "{SCRIPTS}/Parallel_prodigal.py {threads} {input} -s 100 -o {params.dir} -T {params.dir}/temp_splits"

# ----- get rpsblast annotation
rule Batch_rpsblast:
    input:   "{path}.faa"
    output:  "{path}.cogs.tsv"
    log:     "{path}_cog.log"
    params:  db = COG_DB
    shell:   """
             rpsblast+ -outfmt '6 qseqid sseqid evalue pident length slen qlen' -evalue 0.00001 -query {input} -db {params.db} -out {output} &>{log}
             """

#------- select best hit and use criterion : min 5% coverage, min 1e-10 evalue--------------
rule parse_cogs_annotation:
    input:   Batch=expand("{{path}}/temp_splits/Batch_{nb}.cogs.tsv",nb=range(100))
    output:  cog="{path}/contigs_cogs_best_hits.tsv",
             cat=temp("{path}/contigs_Cog.out")
    shell:   """
             cat {input} > {output.cat}   
             {SCRIPTS}/Filter_Cogs.py {output.cat} --cdd_cog_file {SCG_DATA}/cdd_to_cog.tsv  > {output.cog}
             """

#------- extract scg --------------
rule extract_SCG_sequences:
    input:  annotation="{filename}_cogs_best_hits.tsv",
            gff="{filename}.gff",
            fna="{filename}.fna"
    output: "{filename}_SCG.fna"
    shell:  "{SCRIPTS}/Extract_SCG.py {input.fna} {input.annotation} {SCG_DATA}/scg_cogs_min0.97_max1.03_unique_genera.txt {input.gff}>{output}"

rule cluster_SCG:
    input:  "{path}/{name}_SCG.fna"
    output: "{path}/{name}_{pid}_mmseqs_cluster.tsv"
    params: tmp = "{path}/mmseq_tmp_{pid}",
            out = "{path}/{name}_{pid}_mmseqs"
    threads: 10
    shell:  "mmseqs easy-cluster {input} {params.out} {params.tmp} --min-seq-id {wildcards.pid} -c 0.8 --cov-mode 1 --alignment-mode 3 --threads {threads}"

rule scg_cluster_def:
    input: clu = "{path}/contigs_{pid}_mmseqs_cluster.tsv",
           scg = "{path}/contigs_SCG.fna"
    output: "{path}/SCG_cluster_{pid}.tsv"
    run:
        # get orf to SCG
        orf_to_scg = {header.split()[0]:header.split()[1] for header,_ in sfp(open(input["scg"]))}

        # get clu def
        orf_to_clu = defaultdict(lambda :defaultdict(list))
        for line in open(input["clu"]):
            rep,orf = line.rstrip().split("\t")
            orf_to_clu[orf_to_scg[rep]][rep].append(orf)

        # output
        with open(output[0],"w") as handle:
            handle.writelines("%s\t%s\t%s\n"%(cog,index,"\t".join(orfs)) for cog,clus in orf_to_clu.items() for index,(ref,orfs) in enumerate(clus.items()))


# ----- run plasmidnet
rule plasmidnet:
    input: "{path}/Batch_{nb}.faa"
    output:  "{path}/Batch_{nb}_contig_results.tab"
    conda: "%s/plasmidnet.yaml"%CONDA_ENV
    threads: 1
    shell: "python {PLASMIDNET}/bin/plasmidnet.py -f {input} -o $(dirname {output}) -m {PLASMIDNET}/model.zip -j {threads}"

rule generate_result_file:
    input: expand("{{path}}/annotation/temp_splits/Batch_{nb}_contig_results.tab",nb=range(100))
    output: results = "{path}/plasmidnet/results.tsv"
    run:
        with open(output["results"],"w") as handle:
            handle.write("contig\tbacterial_genes\tplasmid_genes\tgenes_in_total\tvote\n")
            for file in input:
                handle.writelines(line for index,line in enumerate(open(file)) if index>0)

# ----- run virsorter2
# to run virsorter 2, you need a db installed inside the env, you can't use it directly...
# conda activate virsort2

rule virsorter2:
    input: "{path}/contigs/contigs.fa"
    output: "{path}/virsorter2/final-viral-boundary.tsv"
    threads: 10
    priority: -1
    shell: """
           set +eu
           source {CONDA}/bin/activate {VIRSORTER_ENV}
           set -eu
           virsorter run -w $(dirname {output}) -i {input} --min-length 1500 -j {threads} all
           """

# ----- 16S annotation 

rule identify_rRNA:
    input: contigs = "{path}/contigs/contigs.fa",
    output: rrna = "{path}/annotation/barrnap_rrna.gff3"
    conda : "%s/barrnap.yaml"%CONDA_ENV
    threads: 100
    shell: "barrnap --threads {threads} {input} > {output}"


rule extract_rna_seq:
    input: contigs = "{path}/contigs/contigs.fa",
           gff = "{path}/annotation/barrnap_rrna.gff3"
    output: seq = "{path}/annotation/16S_seqs.fa",
            bed = "{path}/annotation/16S_bed.tsv"
    run:
        # get 16S bed
        bed = []
        contig_regions = defaultdict(list)
        for line in open(input["gff"]):
            if "16S" in line:
                sline = line.rstrip().split("\t")
                bed.append([sline[0],sline[3],sline[4]])
                contig_regions[sline[0]].append([int(sline[3]),int(sline[4])])
        with open(output["bed"],"w") as handle:
            handle.writelines("%s\n"%"\t".join(line) for line in bed)
        
        # get 16S seq 
        with open(input["contigs"]) as handle, open(output["seq"],"w") as handle_w:
            for header,seq in sfp(handle):
                if header in contig_regions:
                    for start,end in contig_regions[header]:
                        handle_w.write(">%s_%s...%s\n%s\n"%(header,start,end,seq[start:end]))


rule BLCA:
    input: contigs = "{path}/annotation/16S_seqs.fa"
    output: "{path}/annotation/16S_blca.tsv"
    threads: 50
    shell: "python {BLCA}/2.blca_main.py -i {input.contigs} --tax {BLCA}/db_gg/gg_13_5_taxonomy.taxonomy -p {threads} --db {BLCA}/db_gg/gg_13_5 -o {output}"


# ----- run diamong annotation
rule diamond:
    input :  "{filename}.faa"
    output : "{filename}_{annotation}.m8"
    params : db=lambda w: DIAMOND[w.annotation]["db"],
    log:     "{filename}_{annotation}_diamond.log"
    threads:  1000
    shell :   "diamond blastp --more-sensitive -d {params.db}  -q {input} -p {threads} -o {output} -f6 qseqid sseqid qstart qend qlen sstart send slen length pident evalue bitscore &>{log}"

# ----- get inti1 annotation
rule annotation_diamond:
    input :  "{path}/annotation/{filename}_{annotation}.m8"
    output : "{path}/annotation/{filename}_{annotation}_best_hits.tsv"
    params : annotation=lambda w: DIAMOND[w.annotation]["annotation"],
             Bitscore=lambda w: DIAMOND[w.annotation]["filter"][0],
             Evalue=lambda w: DIAMOND[w.annotation]["filter"][1],
             PID=lambda w: DIAMOND[w.annotation]["filter"][2],
             subject_pid=lambda w: DIAMOND[w.annotation]["filter"][3],
             subject_coverage=lambda w: DIAMOND[w.annotation]["filter"][4],
             querry_coverage=lambda w: DIAMOND[w.annotation]["filter"][5],
    shell :   "{SCRIPTS}/M8_Filtering.py {input} -D {params.annotation} -B {params.Bitscore} -E {params.Evalue} -P {params.PID} -R {params.subject_pid} -C {params.subject_coverage} -Q {params.querry_coverage}  >{output}"



# ------ get contigs quality -----------
rule bogus_bed:
    input:   contig="{group}/contigs/contigs.fa"
    output:  bed="{group}/annotation/contigs.bed"
    run :
        handle=open(output['bed'],"w")
        for header,seq in sfp(open(input["contig"])) :
            name=header.split(" ")[0]
            handle.write("\t".join([name,"0",str(len(seq)),name+"\n"]))
        handle.close()


rule get_component_quality:
    input: scgs = "{path}/annotation/SCG_cluster_{pid}.tsv",
           cogs = "{path}/annotation/contigs_cogs_best_hits.tsv",
           cont_len = "{path}/annotation/contigs.bed"
    output: summary = "{path}/annotation/summary_{pid}.tsv"
    run:
        # get component
        contig_to_len = {line.rstrip().split("\t")[0]:int(line.rstrip().split("\t")[2]) for line in open(input["cont_len"])}

        # get scgs clusters
        orfs_to_cog = {}
        cog_to_orfs = defaultdict(list)
        for line in open(input["scgs"]):
            cog = "_".join(line.rstrip().split("\t")[0:2])
            for orf in line.rstrip().split("\t")[2:]:
                orfs_to_cog[orf] = cog
                cog_to_orfs[cog].append(orf)

        # get scg cog annotation
        scg_to_cog = {line.rstrip().split("\t")[0]:line.rstrip().split("\t")[1] for index,line in enumerate(open(input["cogs"])) if index>0}

        # build mapping of scg to contigs
        contigs_to_scgs = defaultdict(set)
        for orf,cog in orfs_to_cog.items():
            contig = "_".join((orf.split('_')[:-1]))
            contigs_to_scgs[contig].add(cog)

        # get completion,contamination
        get_cogs = lambda x:Counter([el.split("_")[0] for el in contigs_to_scgs[x]])
        unique = lambda x:sum([val==1 for val in get_cogs(x).values()])/36.
        contamination = lambda x:(sum(get_cogs(x).values())-len(get_cogs(x)))/36.
        completion = lambda x:len(get_cogs(x))/36.

        sorted_contigs = sorted(contig_to_len.keys(),key=lambda x:-contig_to_len[x])
        with open(output["summary"],"w") as handle:
            handle.write("contig\tuniq_scg\tcomp\tcont\tnuc_size\n")
            handle.writelines("%s\t%s\t%s\t%s\t%s\n"%(contig,"{:.0%}".format(unique(contig)),"{:.0%}".format(completion(contig)),"{:.0%}".format(contamination(contig)),contig_to_len[contig]) for contig in sorted_contigs)
