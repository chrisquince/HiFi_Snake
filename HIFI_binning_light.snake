import os
import glob
from Bio.SeqIO.FastaIO import SimpleFastaParser as sfp
from Bio.SeqIO.QualityIO import FastqGeneralIterator as fgi
from os.path import basename,dirname,realpath,abspath
from collections import defaultdict,Counter
import pysam
import gzip
import numpy as np


# ----------- Run dependant variables ----------- 
# you needs reads, short & long, the graph, as well as an assembly and corresponding orfs/cogs annotation
ROOT = config["ROOT"]
SAMPLE_DIR = config["SAMPLE_DIR"]
CONTIGS = config["CONTIGS"]

# snakemake -s /mnt/gpfs/seb/Applications/Snakescripts/HIFI_binning.snake --config ROOT="/mnt/gpfs/seb/Project/temp/HIFI_annotation/Mouse77_ctg_new" SAMPLE_DIR="/mnt/gpfs/chris/MouseHiFi/Samples/Mouse77" SAMPLE_SHORT='/home/sebr/seb/Project/temp/EI_MOCKS2/data_for_strong/77*'

# snakemake -s /mnt/gpfs/seb/Applications/Snakescripts/HIFI_binning.snake --config ROOT="/mnt/gpfs/seb/Project/temp/HIFI_annotation/Mouse77_utg_new" SAMPLE_DIR="/mnt/gpfs/chris/MouseHiFi/Samples/Mouse77" SAMPLE_SHORT='/home/sebr/seb/Project/temp/EI_MOCKS2/data_for_strong/77*' --cores 100

# snakemake -s /mnt/gpfs/seb/Applications/Snakescripts/HIFI_binning.snake --config ROOT="/mnt/gpfs/cyanoProject/HiFiResults/CP1_CP2_coassembly_contigs" SAMPLE_DIR="/mnt/gpfs/chris/Projects/cyanoProject/HiFi" SAMPLE_SHORT='/mnt/gpfs/cyanoProject/sarahdata_2021_analysis/data/WGS/strong_folder/*'



# binning based on hifi reads
SAMPLES = {basename(dirname(file)).replace(".fastq.gz",""):file for file in glob.glob('%s/*/*.fastq*'%SAMPLE_DIR)}
# chris thing
SAMPLES.update({basename(file).split(".fastq")[0]:file for file in glob.glob('%s/*.fastq'%SAMPLE_DIR)})
SAMPLES.update({basename(file).split(".fastq.gz")[0]:file for file in glob.glob('%s/*.fastq.gz'%SAMPLE_DIR)})


# -------------------------------------------- 

SCRIPTS = "/mnt/gpfs2/seb/Project/Metahood/scripts"
SCRIPTS2 = "/mnt/gpfs2/seb/Applications/scripts"
CSCRIPTS = "/mnt/gpfs2/chris_q/Applications/scripts"

MIN_CONTIG_SIZE_METABAT2=1500
MIN_CONTIG_SIZE = 1000
TASK_MEMORY = 200
SCG_DATA = "/mnt/gpfs2/seb/Project/Metahood/scg_data"
THREADS = 50
MAX_BIN_NB=2000



# -------------- real start of the snakemake --------------
wildcard_constraints:
    sample = "|".join(SAMPLES.keys()),
    binner = "|".join(["concoct","consensus","metabat2"]),
    type = "|".join(["contigs","orfs","contigs_C10K"])


rule results:
    input: expand("%s/profile/mag_{binner}_percent_mapped.tsv"%ROOT,binner=["concoct","consensus","metabat2"])


rule filter_circ_mags:
    input: summary = "{path}/annotation/summary_0.99.tsv",
           cont = "{path}/contigs/contigs.fa"
    output: mag = "{path}/MAGs/mags/circ.done",
            filt = "{path}/contigs/contigs_filt.fa"
    run:
        folder = dirname(output["mag"])
        shell("{CSCRIPTS}/FilterContigs.py {input.cont} {input.summary} %s {output.filt} > {output.mag}"%folder)
        

rule filter_circ_cont:
    input: depth = "{path}/map_hifi/depth.txt",
           cov = "{path}/profile/coverage_contigs_C10K.tsv",
           bed = "{path}/annotation/contigs_C10K.bed",
           circ = "{path}/MAGs/mags/circ.done"
    output: depth = "{path}/map_hifi/no_circ_depth.txt",
            cov = "{path}/profile/coverage_no_circ_C10K.tsv",
    run:
        circ = {header for file in glob.glob("%s/circ*.fa"%dirname(input["circ"])) for header,seq in sfp(open(file))}

        # depth:
        with open(input['depth']) as handle, open(output["depth"],"w") as handle_w:
            for line in handle:
                if line.rstrip().split("\t")[0] in circ:
                    continue
                handle_w.write(line)

        # cov:
        with open(input['cov']) as handle, open(output["cov"],"w") as handle_w:
            nb_dots = {line.rstrip().split("\t")[0].count(".") for line in open(input["bed"])}
            assert len(nb_dots)==1, "nb dots changes, there is more than one nb of dots in all contigs"
            nb_dots = list(nb_dots)[0]
            for line in handle:
                cont = line.rstrip().split("\t")[0]
                if cont.count(".")==nb_dots:
                    contig = cont
                else:
                    contig = ".".join(cont.split(".")[:-1])
                if contig in circ:
                    continue
                handle_w.write(line)


# ------------ map long read to assembly ----------------
rule minimap2:
    input: contigs = "{path}/contigs/contigs.fa",
           samples = lambda x:SAMPLES[x.sample]
    output: "{path}/map_hifi/{sample}_mapped_sorted.bam"
    log:"{path}/map_hifi/{sample}.log"
    threads: 20
    shell: "minimap2 -ax asm10 -I 100g -t {threads} {input.contigs} {input.samples} 2> {log} | samtools view  -b -F 4 -@{threads} - | samtools sort -@{threads} - > {output} "

rule bam_to_paf:
    input: '{path}.bam'
    output: '{path}.paf'
    shell: "samtools view -h {input} | paftools.js sam2paf - > {output}"

rule filter_paf:
    input: paf = expand("{{path}}/map_hifi/{sample}.paf",sample=SAMPLES)
    output: "{path}/profile/hifi_coverage.tsv"
    log:"{path}/map_hifi/cov.log"
    threads: 50
    shell: "{SCRIPTS2}/Minimap_Cov_Mean.py {graph.gfa} {input.paf} > {output}"
 
# ------------ map long read to assembly ----------------
rule bedtools:
    input:   bam="{group}/map_hifi/{sample}_mapped_sorted.bam",
             bed="{group}/annotation/{type}.bed"
    output:  "{group}/map_hifi/{sample}_{type}.cov"
    log:      "{group}/map_hifi/{sample}_{type}.log"
    shell:   "bedtools coverage -a {input.bed} -b {input.bam} -mean > {output} 2>{log} "

rule coverage:
    input: expand("{{path}}/map_hifi/{sample}_{{type}}.cov",sample=SAMPLES)
    output:  "{path}/profile/coverage_{type}.tsv"
    shell : "{SCRIPTS}/collate_coverage.py -o {output} -s _{wildcards.type}.cov -l {input}" 

#--------- Concoct -----------
def get_initial_number_of_bins(file):
    nb_bin=int(2*np.median(list(Counter([header.split(" ")[1] for header,seq in sfp(open(file))]).values())))
    return min(nb_bin,MAX_BIN_NB)

rule cut_contigs:
    input:  fa="{group}/contigs/contigs.fa",
            gff="{group}/annotation/contigs.gff"
    output: contig="{group}/contigs/contigs_C10K.fa",
            Contig_bed="{group}/annotation/contigs_C10K.bed"
    priority: 50
    message:"Use orfs annotation to cut contigs"
    shell:  """{SCRIPTS}/Use_orf_to_cut.py {input.fa} {input.gff} {output.contig} {output.Contig_bed}"""

rule concoct:
    input:   cov="{group}/profile/coverage_no_circ_C10K.tsv",
             fasta="{group}/contigs/contigs_C10K.fa",
             SCG="{group}/annotation/contigs_SCG.fna"
    output:  cluster="{group}/binning/concoct/clustering_gt"+str(MIN_CONTIG_SIZE)+".csv",
             Data="{group}/binning/concoct/original_data_gt%d.csv"%MIN_CONTIG_SIZE
    log: "{group}/binning/concoct/log.txt"
    params:  min_contig_size=MIN_CONTIG_SIZE
    threads: 20
    # I don't want to use checkpoints, (buggy and increase DAG resolution time?) so I'll run the code inside a python run
    run :
        print(input["SCG"])
        nb_bin_init = get_initial_number_of_bins(input["SCG"])
        shell("concoct --coverage_file {input.cov} -i 1000 --composition_file {input.fasta} -b {wildcards.group}/binning/concoct -c %s -l {params.min_contig_size} -t {threads} &>> {log}"%nb_bin_init)


#--------- Get SCG cluster def -------------
rule scg_cluster_def:
    input: clu = "{path}/contigs_0.99_mmseqs_cluster.tsv",
           scg = "{path}/contigs_SCG.fna"
    output: "{path}/SCG_cluster_0.99.tsv"
    run:
        # get orf to SCG
        orf_to_scg = {header.split()[0]:header.split()[1] for header,_ in sfp(open(input["scg"]))}

        # get clu def
        orf_to_clu = defaultdict(lambda :defaultdict(list))
        for line in open(input["clu"]):
            rep,orf = line.rstrip().split("\t")
            orf_to_clu[orf_to_scg[rep]][rep].append(orf)

        # output
        with open(output[0],"w") as handle:
            handle.writelines("%s\t%s\t%s\n"%(cog,index,"\t".join(orfs)) for cog,clus in orf_to_clu.items() for index,(ref,orfs) in enumerate(clus.items()))



#--------- Get SCG table out of clustering -------------
rule SCG_table:
    input  : bins="{group}/binning/{binner}/clustering_{name}.csv",
             SCG="{group}/annotation/contigs_SCG.fna",
             orf_bed="{group}/annotation/orf.bed",
             split_bed="{group}/annotation/contigs_C10K.bed",
             clu = "{group}/annotation/SCG_cluster_0.99.tsv"
    output : "{group}/binning/{binner}/{name}_SCG_table.csv"
    shell  : "{SCRIPTS}/SCG_in_Bins.py {input.bins} {input.SCG} {input.orf_bed} {input.split_bed} -t {output} -c {input.clu}"


#--------- Concoct refine -------------

rule refine:
    input:  bins="{group}/binning/concoct/clustering_gt%d.csv"%MIN_CONTIG_SIZE,
            table="{group}/binning/concoct/gt%d_SCG_table.csv"%MIN_CONTIG_SIZE,
            SCG="{group}/annotation/contigs_SCG.fna",
            Data="{group}/binning/concoct/original_data_gt%d.csv"%MIN_CONTIG_SIZE
    output: bins="{group}/binning/concoct/clustering_refine.csv",
    params: temp="{group}/binning/concoct/refine.temp",
            path = "{group}/binning/concoct"
    log:    temp("{group}/binning/concoct/clustering.log")
    threads: 20
    shell:  """
            DATA=$(realpath {input.Data})
            TABLE=$(realpath {input.table})
            TEMP=$(realpath {params.temp})
            LOG=$(realpath {log})
            sed '1d' {input.bins} > $TEMP
            cd {params.path}
            concoct_refine $TEMP $DATA $TABLE -t {threads} &>>$LOG
            rm $TEMP
            """

#--------- merge back contigs --------------------------
rule merge_contigs:
    input:   refine="{path}clustering_refine.csv",
             table ="{path}refine_SCG_table.csv" 
    output:  "{path}clustering_concoct.csv"
    log:     "{path}clustering_consensus.log"
    shell:   "{SCRIPTS}/Consensus.py {input.refine} >{output} 2>{log}"

#--------- estimate the number of mag --------------------

rule output_number_of_mag:
    input:   table="{path}/{binner}_SCG_table.csv"
    output:  mag_nb="{path}/{binner}_MAG_nb.txt",
             mag_list="{path}/{binner}_MAG_list.txt"
    run:
        with open(output["mag_nb"],"w") as handle_nb:
            with open(output["mag_list"],"w") as handle_list:
                nb=0
                mags=[]
                for index,line in enumerate(open(input["table"])) :
                    if index==0:
                        continue
                    split_line=line.rstrip().split(',')
                    if sum([element=="1" for element in split_line[1:]])>=(0.75*36) :
                        nb+=1
                        mags.append(split_line[0])
                handle_nb.write(str(nb)+"\n")
                handle_list.write("\n".join([nb for nb in mags]))

#-- produce a contig file for each bins be they good or not (metabat2 style) --
ruleorder : metabat2>output_bins
rule output_bins:
    input:   contigs="{group}/contigs/contigs.fa",
             clustering="{group}/binning/{binner}/clustering_{binner}.csv"
    output:  "{group}/binning/{binner}/bins/done"
    shell :"""
    {SCRIPTS}/Split_fasta_by_bin.py {input.clustering} $(dirname {output}) --fasta {input.contigs}
    touch {output}
    """

#-- create a consensus binning between concoct and metabat2 --

rule get_consensus_binning :
    # only support 2 binner as of now
    input : c_bin_def = "{group}/binning/concoct/clustering_concoct.csv",
            m_bin_def = "{group}/binning/metabat2/clustering_metabat2.csv",
            c_mag_list = "{group}/binning/concoct/concoct_MAG_list.txt",
            m_mag_list = "{group}/binning/metabat2/metabat2_MAG_list.txt",
            scg = "{group}/annotation/contigs_SCG.fna",
            contig_profiles = "{group}/binning/concoct/original_data_gt%s.csv"%MIN_CONTIG_SIZE,
            contig_bed = "{group}/annotation/contigs.bed"
    output : "{group}/binning/consensus/clustering_consensus.csv"
    shell :"""
    {SCRIPTS}/consensus_binning.py -c_bin_def {input.c_bin_def} -m_bin_def {input.m_bin_def} -c_mag_list {input.c_mag_list} -m_mag_list {input.m_mag_list} -scg {input.scg} -contig_profiles {input.contig_profiles} -contig_bed {input.contig_bed} -o {output}
    """



rule nb_nuc:
    output: "{path}/profile/{sample}_nb_nuc.tsv"
    params: lambda x:SAMPLES[x.sample]
    shell: "seqtk fqchk {params}| grep ALL | cut -f2 > {output}"

rule create_normalisation:
    input:  expand("{{path}}/profile/{sample}_nb_nuc.tsv",sample=SAMPLES)
    output: "{path}/profile/Normalisation.tsv"
    run:
        samples_nuc = {}
        for sample in SAMPLES:
            file = "%s/profile/%s_nb_nuc.tsv"%(ROOT,sample)
            samples_nuc[sample] = int(next(open(file)).rstrip())
        sorted_samples = sorted(samples_nuc.keys())
        with open(output[0],"w") as handle:
            handle.write("Normalisation\t%s\n"%"\t".join(sorted_samples))
            handle.write("Nucleotides\t%s\n"%("\t".join([str(samples_nuc[sample]) for sample in sorted_samples])))
            handle.write("median_scg\t%s\n"%("\t".join(["0" for sample in sorted_samples])))


rule mag_coverage:
    input: mags="{group}/binning/{binner}/{binner}_MAG_list.txt",
           cluster="{group}/binning/{binner}/clustering_{binner}.csv",
           cov="{group}/profile/coverage_contigs.tsv",
           len="{group}/annotation/contigs.bed",
           nb_nuc="{group}/profile/Normalisation.tsv"
    output: mag_cov="{group}/profile/mag_{binner}_coverage.tsv",
            mag_map="{group}/profile/mag_{binner}_percent_mapped.tsv"
    shell:"""
    {SCRIPTS}/mag_coverage.py -m {input.mags} -c {input.cluster} -t {input.cov} -l {input.len} -n {input.nb_nuc} -v {output.mag_cov} -p {output.mag_map}
    """

# ------------ metabat2 ------------
rule generate_depth :
    input:  expand("{{group}}/map_hifi/{sample}_mapped_sorted.bam",sample=SAMPLES)
    output: "{group}/map_hifi/depth.txt"
    log : "{group}/map_hifi/depth.log"
    shell: "jgi_summarize_bam_contig_depths --outputDepth {output} {input} &>{log}" 

rule metabat2 :
    input:  contig="{group}/contigs/contigs.fa",
            depth="{group}/map_hifi/no_circ_depth.txt"
    output: "{group}/binning/metabat2/bins/done"
    params: out="{group}/binning/metabat2/bins/bin",
            min_contig_size=max(1500,MIN_CONTIG_SIZE_METABAT2) # metabat2 does not bin anything smaller than 1500
    threads : 20
    shell: """metabat2 -i {input.contig} -a {input.depth} -t {threads} -o {params.out} -m {params.min_contig_size}
            touch {output}
            """

rule renames_metabat2_bins :
    input: "{group}/binning/metabat2/bins/done"
    output:"{group}/binning/metabat2/bins/name_done"
    run:
        List_bins=glob.glob(wildcards.group+"/binning/metabat2/bins/bin*.fa")
        for bin_ in List_bins:
            new_name=bin_.replace("bin.","Bin_")
            os.system("mv %s %s"%(bin_,new_name))
        os.system("touch %s"%output)

rule post_processing :
    input: "{group}/binning/metabat2/bins/name_done"
    output:"{group}/binning/metabat2/clustering_metabat2.csv"
    run:
        List_bins=glob.glob(wildcards.group+"/binning/metabat2/bins/Bin*.fa")
        Handle=open(output[0],"w")
        Handle.write("contig_id,0\n")
        for file in List_bins :
            bin_name=file.split("Bin_")[-1].split('.fa')[0]
            for name,seq in sfp(open(file)) :
                Handle.write(",".join([name,bin_name])+"\n")
        Handle.close()





# ----- run gtdb -----------
STR_SCRIPTS = "/mnt/gpfs/seb/Project/STRONG/SnakeNest/scripts"
rule mag_directory:
    input: clustering = "{path}/binning/{binner}/clustering_{binner}.csv",
           assembly = "{path}/contigs/contigs.fa",
           mag_list = "{path}/binning/{binner}/{binner}_MAG_nb.txt"
    output: mags = "{path}/MAGs/mags/{binner}.done"
    shell: """
           {STR_SCRIPTS}/Split_fasta_by_bin.py {input.assembly} {input.clustering} $(dirname {output})
           touch {output}
           """

rule gtdb :
    input : "{path}/mags/consensus.done"
    output : "{path}/gtdb/gtdbtk.ar122.classify.tree"
    threads : 50
    shell : """
    gtdbtk classify_wf --cpus {threads} --genome_dir $(dirname {input}) --out_dir $(dirname {output}) --extension .fa
    touch {output}
    """
